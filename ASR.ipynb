{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/w8091032/re0-web/blob/master/ASR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBpUJ6-X9hnw",
        "outputId": "cdbf82c8-ca4c-48e4-bbf7-192790bb9d10"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (2.14.4)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.11/dist-packages (3.1.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.11/dist-packages (0.13.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.3.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n",
            "Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets) (2025.3.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.32.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: click>=8.1.8 in /usr/local/lib/python3.11/dist-packages (from jiwer) (8.2.1)\n",
            "Requirement already satisfied: rapidfuzz>=3.9.7 in /usr/local/lib/python3.11/dist-packages (from jiwer) (3.13.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.13.2)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile) (2.22)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.20.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.18.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (1.1.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->datasets) (2025.4.26)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Requirement already satisfied: bitsandbytes in /usr/local/lib/python3.11/dist-packages (0.46.0)\n",
            "Requirement already satisfied: transformers==4.50.0 in /usr/local/lib/python3.11/dist-packages (4.50.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.26.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0) (0.32.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers==4.50.0) (4.67.1)\n",
            "Requirement already satisfied: torch<3,>=2.2 in /usr/local/lib/python3.11/dist-packages (from bitsandbytes) (2.6.0+cu124)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.0) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.0) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.26.0->transformers==4.50.0) (1.1.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.0) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.0) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.0) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers==4.50.0) (2025.4.26)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n",
            "Requirement already satisfied: ctranslate2==4.4.0 in /usr/local/lib/python3.11/dist-packages (4.4.0)\n",
            "Requirement already satisfied: whisperx in /usr/local/lib/python3.11/dist-packages (3.3.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from ctranslate2==4.4.0) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from ctranslate2==4.4.0) (2.0.2)\n",
            "Requirement already satisfied: pyyaml<7,>=5.3 in /usr/local/lib/python3.11/dist-packages (from ctranslate2==4.4.0) (6.0.2)\n",
            "Requirement already satisfied: faster-whisper>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from whisperx) (1.1.1)\n",
            "Requirement already satisfied: nltk>=3.9.1 in /usr/local/lib/python3.11/dist-packages (from whisperx) (3.9.1)\n",
            "Requirement already satisfied: onnxruntime>=1.19 in /usr/local/lib/python3.11/dist-packages (from whisperx) (1.22.0)\n",
            "Requirement already satisfied: pandas>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from whisperx) (2.3.0)\n",
            "Requirement already satisfied: pyannote-audio>=3.3.2 in /usr/local/lib/python3.11/dist-packages (from whisperx) (3.3.2)\n",
            "Requirement already satisfied: torch>=2.5.1 in /usr/local/lib/python3.11/dist-packages (from whisperx) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchaudio>=2.5.1 in /usr/local/lib/python3.11/dist-packages (from whisperx) (2.6.0+cu124)\n",
            "Requirement already satisfied: transformers>=4.48.0 in /usr/local/lib/python3.11/dist-packages (from whisperx) (4.50.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.13 in /usr/local/lib/python3.11/dist-packages (from faster-whisper>=1.1.1->whisperx) (0.32.2)\n",
            "Requirement already satisfied: tokenizers<1,>=0.13 in /usr/local/lib/python3.11/dist-packages (from faster-whisper>=1.1.1->whisperx) (0.21.1)\n",
            "Requirement already satisfied: av>=11 in /usr/local/lib/python3.11/dist-packages (from faster-whisper>=1.1.1->whisperx) (14.4.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from faster-whisper>=1.1.1->whisperx) (4.67.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9.1->whisperx) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9.1->whisperx) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk>=3.9.1->whisperx) (2024.11.6)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.19->whisperx) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.19->whisperx) (25.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.19->whisperx) (24.2)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.19->whisperx) (5.29.5)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.19->whisperx) (1.13.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->whisperx) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->whisperx) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=2.2.3->whisperx) (2025.2)\n",
            "Requirement already satisfied: asteroid-filterbanks>=0.4 in /usr/local/lib/python3.11/dist-packages (from pyannote-audio>=3.3.2->whisperx) (0.4.0)\n",
            "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pyannote-audio>=3.3.2->whisperx) (0.8.1)\n",
            "Requirement already satisfied: lightning>=2.0.1 in /usr/local/lib/python3.11/dist-packages (from pyannote-audio>=3.3.2->whisperx) (2.5.1.post0)\n",
            "Requirement already satisfied: omegaconf<3.0,>=2.1 in /usr/local/lib/python3.11/dist-packages (from pyannote-audio>=3.3.2->whisperx) (2.3.0)\n",
            "Requirement already satisfied: pyannote.core>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote-audio>=3.3.2->whisperx) (5.0.0)\n",
            "Requirement already satisfied: pyannote.database>=5.0.1 in /usr/local/lib/python3.11/dist-packages (from pyannote-audio>=3.3.2->whisperx) (5.1.3)\n",
            "Requirement already satisfied: pyannote.metrics>=3.2 in /usr/local/lib/python3.11/dist-packages (from pyannote-audio>=3.3.2->whisperx) (3.2.1)\n",
            "Requirement already satisfied: pyannote.pipeline>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from pyannote-audio>=3.3.2->whisperx) (3.0.1)\n",
            "Requirement already satisfied: pytorch-metric-learning>=2.1.0 in /usr/local/lib/python3.11/dist-packages (from pyannote-audio>=3.3.2->whisperx) (2.8.1)\n",
            "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote-audio>=3.3.2->whisperx) (13.9.4)\n",
            "Requirement already satisfied: semver>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote-audio>=3.3.2->whisperx) (3.0.4)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from pyannote-audio>=3.3.2->whisperx) (0.13.1)\n",
            "Requirement already satisfied: speechbrain>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote-audio>=3.3.2->whisperx) (1.0.3)\n",
            "Requirement already satisfied: tensorboardX>=2.6 in /usr/local/lib/python3.11/dist-packages (from pyannote-audio>=3.3.2->whisperx) (2.6.2.2)\n",
            "Requirement already satisfied: torch-audiomentations>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from pyannote-audio>=3.3.2->whisperx) (0.12.0)\n",
            "Requirement already satisfied: torchmetrics>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from pyannote-audio>=3.3.2->whisperx) (1.7.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->whisperx) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->whisperx) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->whisperx) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->whisperx) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->whisperx) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->whisperx) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->whisperx) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->whisperx) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->whisperx) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->whisperx) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->whisperx) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->whisperx) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->whisperx) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->whisperx) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->whisperx) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->whisperx) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->whisperx) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->whisperx) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.5.1->whisperx) (3.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.19->whisperx) (1.3.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers>=4.48.0->whisperx) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers>=4.48.0->whisperx) (0.5.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.13->faster-whisper>=1.1.1->whisperx) (1.1.2)\n",
            "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.11/dist-packages (from lightning>=2.0.1->pyannote-audio>=3.3.2->whisperx) (0.14.3)\n",
            "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.11/dist-packages (from lightning>=2.0.1->pyannote-audio>=3.3.2->whisperx) (2.5.1.post0)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from omegaconf<3.0,>=2.1->pyannote-audio>=3.3.2->whisperx) (4.9.3)\n",
            "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.11/dist-packages (from pyannote.core>=5.0.0->pyannote-audio>=3.3.2->whisperx) (2.4.0)\n",
            "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.core>=5.0.0->pyannote-audio>=3.3.2->whisperx) (1.15.3)\n",
            "Requirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.database>=5.0.1->pyannote-audio>=3.3.2->whisperx) (0.16.0)\n",
            "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote-audio>=3.3.2->whisperx) (1.6.1)\n",
            "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote-audio>=3.3.2->whisperx) (0.6.2)\n",
            "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote-audio>=3.3.2->whisperx) (0.9.0)\n",
            "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pyannote.metrics>=3.2->pyannote-audio>=3.3.2->whisperx) (3.10.0)\n",
            "Requirement already satisfied: optuna>=3.1 in /usr/local/lib/python3.11/dist-packages (from pyannote.pipeline>=3.0.1->pyannote-audio>=3.3.2->whisperx) (4.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas>=2.2.3->whisperx) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.0.0->pyannote-audio>=3.3.2->whisperx) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=12.0.0->pyannote-audio>=3.3.2->whisperx) (2.19.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->pyannote-audio>=3.3.2->whisperx) (1.17.1)\n",
            "Requirement already satisfied: hyperpyyaml in /usr/local/lib/python3.11/dist-packages (from speechbrain>=1.0.0->pyannote-audio>=3.3.2->whisperx) (1.2.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from speechbrain>=1.0.0->pyannote-audio>=3.3.2->whisperx) (0.2.0)\n",
            "Requirement already satisfied: julius<0.3,>=0.2.3 in /usr/local/lib/python3.11/dist-packages (from torch-audiomentations>=0.11.0->pyannote-audio>=3.3.2->whisperx) (0.2.7)\n",
            "Requirement already satisfied: torch-pitch-shift>=1.2.2 in /usr/local/lib/python3.11/dist-packages (from torch-audiomentations>=0.11.0->pyannote-audio>=3.3.2->whisperx) (1.2.5)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.19->whisperx) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.5.1->whisperx) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.48.0->whisperx) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.48.0->whisperx) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.48.0->whisperx) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers>=4.48.0->whisperx) (2025.4.26)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote-audio>=3.3.2->whisperx) (2.22)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio>=3.3.2->whisperx) (3.11.15)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote-audio>=3.3.2->whisperx) (0.1.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote-audio>=3.3.2->whisperx) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote-audio>=3.3.2->whisperx) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote-audio>=3.3.2->whisperx) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote-audio>=3.3.2->whisperx) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote-audio>=3.3.2->whisperx) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote-audio>=3.3.2->whisperx) (3.2.3)\n",
            "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote-audio>=3.3.2->whisperx) (1.16.1)\n",
            "Requirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote-audio>=3.3.2->whisperx) (6.9.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote-audio>=3.3.2->whisperx) (2.0.41)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.17.1->pyannote.metrics>=3.2->pyannote-audio>=3.3.2->whisperx) (3.6.0)\n",
            "Requirement already satisfied: primePy>=1.3 in /usr/local/lib/python3.11/dist-packages (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote-audio>=3.3.2->whisperx) (1.3)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote-audio>=3.3.2->whisperx) (1.5.4)\n",
            "Requirement already satisfied: ruamel.yaml>=0.17.28 in /usr/local/lib/python3.11/dist-packages (from hyperpyyaml->speechbrain>=1.0.0->pyannote-audio>=3.3.2->whisperx) (0.18.12)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio>=3.3.2->whisperx) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio>=3.3.2->whisperx) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio>=3.3.2->whisperx) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio>=3.3.2->whisperx) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio>=3.3.2->whisperx) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio>=3.3.2->whisperx) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote-audio>=3.3.2->whisperx) (1.20.0)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote-audio>=3.3.2->whisperx) (1.1.3)\n",
            "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.11/dist-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=1.0.0->pyannote-audio>=3.3.2->whisperx) (0.2.12)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote-audio>=3.3.2->whisperx) (3.2.2)\n",
            "Hit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:7 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "libcudnn8 is already the newest version (8.9.7.29-1+cuda12.2).\n",
            "libcudnn8-dev is already the newest version (8.9.7.29-1+cuda12.2).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 44 not upgraded.\n"
          ]
        }
      ],
      "source": [
        "#安裝元件\n",
        "!pip install datasets evaluate jiwer librosa soundfile\n",
        "!pip install --upgrade bitsandbytes transformers==4.50.0 accelerate\n",
        "!pip install ctranslate2==4.4.0 whisperx\n",
        "!apt-get update\n",
        "!apt-get install -y libcudnn8 libcudnn8-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wd6kPRdv9pEw",
        "outputId": "ec0ec4b3-4093-4bd0-8dea-904543857af9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "所有函式庫匯入完成。\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "from datasets import Dataset, Audio\n",
        "from transformers import (\n",
        "    WhisperProcessor, # 用於載入預訓練的 processor\n",
        "\n",
        ")\n",
        "from dataclasses import dataclass\n",
        "from typing import Any, Dict, List, Union\n",
        "import evaluate # huggingface evaluate\n",
        "from tqdm import tqdm\n",
        "# from transformers import EarlyStoppingCallback # 微調用\n",
        "from ctranslate2.converters import TransformersConverter # HF 模型轉 CTranslate2\n",
        "import whisperx # WhisperX 用於推斷和時間戳\n",
        "# from transformers import WhisperTokenizer # 通常 WhisperProcessor 已包含 Tokenizer\n",
        "import json\n",
        "import zipfile\n",
        "from jiwer import wer # 計算 WER\n",
        "\n",
        "print(\"所有函式庫匯入完成。\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "wqtVbRds9tPP",
        "outputId": "27b30c07-95c9-4dc6-a04b-1cd7fb61f860"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在測試載入單個音訊檔案: /content/test_audio_1.wav\n",
            "音訊載入成功，取樣率: 16000, 數組長度: 56320\n"
          ]
        }
      ],
      "source": [
        "import librosa\n",
        "\n",
        "def load_and_preprocess_audio(audio_path, target_sampling_rate=16000):\n",
        "    \"\"\"載入音訊檔案，轉換取樣率，並準備給 Whisper Processor 使用。\"\"\"\n",
        "    try:\n",
        "        # 使用 librosa 載入音訊，它會自動處理多種格式\n",
        "        speech_array, sampling_rate = librosa.load(audio_path, sr=None) # sr=None 表示載入原始取樣率\n",
        "\n",
        "        if sampling_rate != target_sampling_rate:\n",
        "            speech_array = librosa.resample(speech_array, orig_sr=sampling_rate, target_sr=target_sampling_rate)\n",
        "            sampling_rate = target_sampling_rate\n",
        "\n",
        "        return {\"array\": speech_array, \"sampling_rate\": sampling_rate}\n",
        "    except Exception as e:\n",
        "        print(f\"載入音訊 [{audio_path}] 失敗: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- Define example_audio_path BEFORE using it ---\n",
        "example_audio_path = \"/content/test_audio_1.wav\" # Or \"/content/test_audio_2.wav\"\n",
        "# Ensure \"test_audio_1.wav\" is actually uploaded to /content/\n",
        "\n",
        "print(f\"正在測試載入單個音訊檔案: {example_audio_path}\")\n",
        "audio_input_data = load_and_preprocess_audio(example_audio_path)\n",
        "\n",
        "if audio_input_data:\n",
        "    print(f\"音訊載入成功，取樣率: {audio_input_data['sampling_rate']}, 數組長度: {len(audio_input_data['array'])}\")\n",
        "else:\n",
        "    print(f\"音訊載入失敗，請檢查路徑或檔案。\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "T-rq_zuDiWs9",
        "outputId": "6499fd87-143c-44ad-c789-9681811b849f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在使用設備: cuda\n"
          ]
        }
      ],
      "source": [
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"正在使用設備: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "14jPDwb99un9",
        "outputId": "9a5ba51b-c5c4-4022-a6f0-06d7d847c4d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base result directory: model_result\n",
            "Task directory: model_result/task1\n",
            "Version directory: model_result/task1/v1_inference\n",
            "HuggingFace Model directory: model_result/task1/v1_inference/model\n",
            "CTranslate2 Model directory: model_result/task1/v1_inference/ct2_model\n"
          ]
        }
      ],
      "source": [
        "def setup_directories(task_name: str = \"task1\", version: str = \"v1_colab_test\"):\n",
        "    \"\"\"建立結果目錄和版本子目錄\"\"\"\n",
        "    base_result_dir = \"model_result\"\n",
        "    task_dir = os.path.join(base_result_dir, task_name)\n",
        "    version_dir = os.path.join(task_dir, version)\n",
        "    model_dir = os.path.join(version_dir, \"model\") # 用於存放 HF 模型\n",
        "    ct2_model_dir = os.path.join(version_dir, \"ct2_model\") # 用於存放 CTranslate2 模型\n",
        "\n",
        "\n",
        "    os.makedirs(task_dir, exist_ok=True)\n",
        "    os.makedirs(version_dir, exist_ok=True)\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    os.makedirs(ct2_model_dir, exist_ok=True)\n",
        "\n",
        "    print(f\"Base result directory: {base_result_dir}\")\n",
        "    print(f\"Task directory: {task_dir}\")\n",
        "    print(f\"Version directory: {version_dir}\")\n",
        "    print(f\"HuggingFace Model directory: {model_dir}\")\n",
        "    print(f\"CTranslate2 Model directory: {ct2_model_dir}\")\n",
        "    return version_dir, model_dir, ct2_model_dir\n",
        "\n",
        "\n",
        "current_version = \"v1_inference\"\n",
        "version_dir, hf_model_save_path, ct2_model_save_path = setup_directories(version=current_version)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "-1rJ_NPxHkIE",
        "outputId": "0faa4042-d801-42bf-f468-fa71fc34d8a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "使用測試音檔 (請確保已上傳): ['/content/test_audio_1.wav', '/content/test_audio_2.wav']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "validation_audio_files = [\n",
        "    \"/content/test_audio_1.wav\",  # Colab 的 /content/ 目錄\n",
        "    \"/content/test_audio_2.wav\"\n",
        "]\n",
        "print(f\"使用測試音檔 (請確保已上傳): {validation_audio_files}\")\n",
        "\n",
        "\n",
        "if 'hf_model_save_path' not in globals():\n",
        "    version_dir, hf_model_save_path, ct2_model_save_path = setup_directories(version=\"v_nozip_test\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "tEoe3qHeDD3X",
        "outputId": "753a0db6-2666-4e6c-c377-75ee403eda12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WhisperProcessor 從 Hugging Face Hub (openai/whisper-base) 載入成功。\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    MODEL_ID_FROM_HUB = \"openai/whisper-base\"\n",
        "    processor = WhisperProcessor.from_pretrained(MODEL_ID_FROM_HUB)\n",
        "    print(f\"WhisperProcessor 從 Hugging Face Hub ({MODEL_ID_FROM_HUB}) 載入成功。\")\n",
        "except Exception as e:\n",
        "    print(f\"從 Hugging Face Hub ({MODEL_ID_FROM_HUB}) 載入 WhisperProcessor 失敗: {e}\")\n",
        "    processor = None\n",
        "\n",
        "def prepare_dataset(audio_files_list: List[str], dummy_texts: List[str] = None):\n",
        "\n",
        "    if dummy_texts is None:\n",
        "        dummy_texts = [\"\"] * len(audio_files_list)\n",
        "    if len(audio_files_list) != len(dummy_texts):\n",
        "        raise ValueError(\"音檔列表和文本列表的長度必須一致!\")\n",
        "    val_dict = { \"audio\": audio_files_list, \"text\": dummy_texts }\n",
        "    dataset = Dataset.from_dict(val_dict)\n",
        "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "    return dataset\n",
        "\n",
        "def prepare_features(batch, current_processor, language_code=\"en\"):\n",
        "\n",
        "    if not current_processor:\n",
        "        raise ValueError(\"Processor 未被初始化!\")\n",
        "    audio = batch[\"audio\"]\n",
        "    batch[\"input_features\"] = current_processor.feature_extractor(\n",
        "        audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], return_tensors=\"pt\"\n",
        "    ).input_features[0]\n",
        "    batch[\"labels\"] = current_processor.tokenizer(\n",
        "        batch[\"text\"], max_length=448, truncation=True, padding=\"max_length\"\n",
        "    ).input_ids\n",
        "    batch[\"file_path\"] = audio[\"path\"]\n",
        "    if audio[\"path\"]:\n",
        "      batch[\"audio_file_name\"] = os.path.splitext(os.path.basename(audio[\"path\"]))[0]\n",
        "    else:\n",
        "      batch[\"audio_file_name\"] = \"unknown_file\"\n",
        "    return batch\n",
        "\n",
        "TARGET_LANGUAGE = \"en\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QCruszKwDFoM",
        "outputId": "a641ac9a-088a-453c-a149-2799c763e140"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WhisperProcessor 從 Hugging Face Hub (openai/whisper-base) 載入成功。\n",
            "目標語言設定為: en\n"
          ]
        }
      ],
      "source": [
        "# 儲存格 6\n",
        "\n",
        "\n",
        "MODEL_ID_FROM_HUB = \"openai/whisper-base\"\n",
        "try:\n",
        "    processor = WhisperProcessor.from_pretrained(MODEL_ID_FROM_HUB)\n",
        "    print(f\"WhisperProcessor 從 Hugging Face Hub ({MODEL_ID_FROM_HUB}) 載入成功。\")\n",
        "except Exception as e:\n",
        "    print(f\"從 Hugging Face Hub ({MODEL_ID_FROM_HUB}) 載入 WhisperProcessor 失敗: {e}\")\n",
        "    processor = None\n",
        "\n",
        "\n",
        "def prepare_dataset(audio_files_list: List[str], dummy_texts: List[str] = None):\n",
        "    \"\"\"\n",
        "    根據音檔路徑列表和可選的文本列表創建 Hugging Face Dataset。\n",
        "    如果 dummy_texts 未提供，則創建空文本。\n",
        "    \"\"\"\n",
        "    if dummy_texts is None:\n",
        "        dummy_texts = [\"\"] * len(audio_files_list)\n",
        "\n",
        "    if len(audio_files_list) != len(dummy_texts):\n",
        "        raise ValueError(\"音檔列表和文本列表的長度必須一致!\")\n",
        "\n",
        "    val_dict = {\n",
        "        \"audio\": audio_files_list,\n",
        "        \"text\": dummy_texts\n",
        "    }\n",
        "    dataset = Dataset.from_dict(val_dict)\n",
        "    dataset = dataset.cast_column(\"audio\", Audio(sampling_rate=16000))\n",
        "    return dataset\n",
        "\n",
        "\n",
        "def prepare_features(batch, current_processor, language_code=\"en\"): # 預設語言為英文\n",
        "    \"\"\"處理音訊並將目標文本編碼為標籤ID\"\"\"\n",
        "    if not current_processor:\n",
        "        raise ValueError(\"Processor 未被初始化!\")\n",
        "\n",
        "    # 處理音訊\n",
        "    audio = batch[\"audio\"] # HuggingFace Dataset 會自動載入音訊\n",
        "\n",
        "    # 計算 log-mel 輸入特徵\n",
        "    batch[\"input_features\"] = current_processor.feature_extractor(\n",
        "        audio[\"array\"],\n",
        "        sampling_rate=audio[\"sampling_rate\"],\n",
        "        # language=language_code, # feature_extractor 通常不需要 language, tokenizer 需要\n",
        "        return_tensors=\"pt\"\n",
        "    ).input_features[0] # 取第一個元素，因為通常 feature_extractor 會返回 list of features\n",
        "\n",
        "\n",
        "    batch[\"labels\"] = current_processor.tokenizer(\n",
        "        batch[\"text\"], # 來自 prepare_dataset 提供的 dummy_text\n",
        "        max_length=448, # 與圖片中的設定一致\n",
        "        truncation=True,\n",
        "        padding=\"max_length\" # 或 \"longest\"\n",
        "    ).input_ids # 通常是 .input_ids\n",
        "\n",
        "    # 提取檔案路徑和檔名 (可選，用於追蹤)\n",
        "    batch[\"file_path\"] = audio[\"path\"]\n",
        "    if audio[\"path\"]: # 避免 audio[\"path\"] is None 的情況\n",
        "      batch[\"audio_file_name\"] = os.path.splitext(os.path.basename(audio[\"path\"]))[0]\n",
        "    else:\n",
        "      batch[\"audio_file_name\"] = \"unknown_file\"\n",
        "\n",
        "    return batch\n",
        "\n",
        "# 語言設定 (請根據你的模型和資料調整，例如 \"en\", \"zh\")\n",
        "TARGET_LANGUAGE = \"en\"\n",
        "print(f\"目標語言設定為: {TARGET_LANGUAGE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "7ae28db8ff9d41e1945a317c9a41c8a6"
          ]
        },
        "id": "h9CX5WZzDFfC",
        "outputId": "982b202a-cc37-40c1-ca09-d6290f9bf39a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "正在使用 2 個音檔準備 'val' 資料集...\n",
            "正在處理 val 資料集...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7ae28db8ff9d41e1945a317c9a41c8a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/2 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "val 資料集處理完成。處理後的欄位: ['input_features', 'labels', 'file_path', 'audio_file_name']\n"
          ]
        }
      ],
      "source": [
        "if 'validation_audio_files' in globals() and validation_audio_files and 'processor' in globals() and processor:\n",
        "    print(f\"正在使用 {len(validation_audio_files)} 個音檔準備 'val' 資料集...\")\n",
        "    raw_val_dataset = prepare_dataset(validation_audio_files)\n",
        "    dataset_dict = {\"val\": raw_val_dataset}\n",
        "\n",
        "    processed_dataset = {}\n",
        "    for split in [\"val\"]:\n",
        "        if split in dataset_dict and len(dataset_dict[split]) > 0: # <--- 新增 len 檢查\n",
        "            print(f\"正在處理 {split} 資料集...\")\n",
        "            processed_dataset[split] = dataset_dict[split].map(\n",
        "                prepare_features,\n",
        "                fn_kwargs={\"current_processor\": processor, \"language_code\": TARGET_LANGUAGE},\n",
        "                remove_columns=dataset_dict[split].column_names,\n",
        "                num_proc=1\n",
        "            )\n",
        "            print(f\"{split} 資料集處理完成。處理後的欄位: {processed_dataset[split].column_names}\")\n",
        "        elif split in dataset_dict and len(dataset_dict[split]) == 0:\n",
        "            print(f\"{split} 資料集為空，跳過 map 處理。\")\n",
        "            processed_dataset[split] = dataset_dict[split] # 保留空的 Dataset\n",
        "        else:\n",
        "            print(f\"找不到 {split} 資料集。\")\n",
        "elif not ('validation_audio_files' in globals() and validation_audio_files):\n",
        "    print(\"提示: `validation_audio_files` 為空，已跳過資料集處理。\")\n",
        "    processed_dataset = {'val': Dataset.from_dict({\"audio\":[], \"text\":[]})} # 創建一個空的processed_dataset結構\n",
        "else:\n",
        "    print(\"錯誤: 'validation_audio_files' 未定義或為空，或者 'processor' 未成功載入。請檢查前面的儲存格。\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "referenced_widgets": [
            "08e1812964d148a38630504f214d6c7f",
            "cf64a9748c704c39b25c57ce11196d81",
            "bdf51a65f82f478598475275e6ff251f"
          ]
        },
        "id": "d3VF3_5WDIS_",
        "outputId": "21df7366-cc6a-427f-8a78-23f7a0eeeb1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "將 HuggingFace Hub 模型 (openai/whisper-base) 轉換為 CTranslate2 格式...\n",
            "開始轉換模型 'openai/whisper-base' 到 CTranslate2 格式...\n",
            "輸出目錄: model_result/task1/v1_inference/ct2_model\n",
            "量化方式: float16\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "08e1812964d148a38630504f214d6c7f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/1.98k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cf64a9748c704c39b25c57ce11196d81",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/290M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bdf51a65f82f478598475275e6ff251f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/3.81k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "模型成功轉換並儲存到 model_result/task1/v1_inference/ct2_model\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def convert_hf_model_to_ct2(model_name_or_path: str,\n",
        "                              output_dir: str,\n",
        "                              quantization: str = \"float16\",\n",
        "                              trust_remote_code: bool = True,\n",
        "                              force: bool = True):\n",
        "\n",
        "    print(f\"開始轉換模型 '{model_name_or_path}' 到 CTranslate2 格式...\")\n",
        "    print(f\"輸出目錄: {output_dir}\")\n",
        "    print(f\"量化方式: {quantization}\")\n",
        "    files_to_copy = ['preprocessor_config.json']\n",
        "\n",
        "    converter = TransformersConverter(\n",
        "        model_name_or_path=model_name_or_path, # 這裡會傳入 Hub ID\n",
        "        # copy_files=files_to_copy, # 當 model_name_or_path 是 Hub ID 時，此參數可能不那麼關鍵或由 converter 自動處理\n",
        "        trust_remote_code=trust_remote_code,\n",
        "    )\n",
        "    converter.convert(output_dir=output_dir, quantization=quantization, force=force)\n",
        "    print(f\"模型成功轉換並儲存到 {output_dir}\")\n",
        "\n",
        "# --- 執行轉換 ---\n",
        "# MODEL_ID_FROM_HUB 來自儲存格 6\n",
        "# ct2_model_save_path 來自儲存格 3\n",
        "print(f\"\\n將 HuggingFace Hub 模型 ({MODEL_ID_FROM_HUB}) 轉換為 CTranslate2 格式...\")\n",
        "try:\n",
        "    convert_hf_model_to_ct2(\n",
        "        model_name_or_path=MODEL_ID_FROM_HUB,\n",
        "        output_dir=ct2_model_save_path,\n",
        "        quantization=\"float16\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"CTranslate2 模型轉換失敗: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "QyAURgDODKf2",
        "outputId": "1903f0e2-db44-4aef-d8c0-3539ff8a7e55"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- 準備 Hugging Face ASR Pipeline ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Device set to use cuda\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hugging Face ASR Pipeline (openai/whisper-base) 載入成功。\n",
            "\n",
            "--- 準備 WhisperX 對齊模型 ---\n",
            "正在為語言 'en' 載入 WhisperX 對齊模型...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/torchaudio/models/wav2vec2_fairseq_base_ls960_asr_ls960.pth\" to /root/.cache/torch/hub/checkpoints/wav2vec2_fairseq_base_ls960_asr_ls960.pth\n",
            "100%|██████████| 360M/360M [00:01<00:00, 192MB/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WhisperX 對齊模型載入成功。\n",
            "\n",
            "--- 開始執行推斷流程 (HF Pipeline + WhisperX Align) ---\n",
            "\n",
            "--- 開始計算輸出 (使用 HF Pipeline + WhisperX Align) ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "處理音訊檔案:   0%|          | 0/2 [00:00<?, ?file/s]/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n",
            "You have passed task=transcribe, but also have set `forced_decoder_ids` to [[1, None], [2, 50359]] which creates a conflict. `forced_decoder_ids` will be ignored in favor of task=transcribe.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "WhisperModel is using WhisperSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `layer_head_mask` not None. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
            "處理音訊檔案:  50%|█████     | 1/2 [00:02<00:02,  2.95s/file]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  處理 test_audio_1 時 WhisperX Align 失敗: tensors used as indices must be long, int, byte or bool tensors. Task 1 使用初步轉錄，Task 2 時間戳將來自初步轉錄 (若有)。\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/whisper/generation_whisper.py:573: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
            "  warnings.warn(\n",
            "Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.\n",
            "處理音訊檔案: 100%|██████████| 2/2 [00:04<00:00,  2.08s/file]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  處理 test_audio_2 時 WhisperX Align 失敗: tensors used as indices must be long, int, byte or bool tensors. Task 1 使用初步轉錄，Task 2 時間戳將來自初步轉錄 (若有)。\n",
            "\n",
            "正在儲存 Task 1 的預測 (2 條) 到: model_result/task1/v1_inference/task1_answer_hf_pipeline_align.txt\n",
            "正在儲存詳細的時間戳資訊到: model_result/task1/v1_inference/val_detailed_timestamps_hf_pipeline_align.json\n",
            "所有預測結果和時間戳已儲存。\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# 儲存格 9: 使用 Hugging Face Pipeline 進行 ASR，並用 WhisperX Align 獲取精確時間戳\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "import json\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "import whisperx # 僅用於載入音訊和對齊\n",
        "from transformers import pipeline # 用於 ASR\n",
        "\n",
        "# --- 檢查並載入必要的變數和模型 ---\n",
        "\n",
        "# 1. 載入 Hugging Face ASR Pipeline (用於初步轉錄)\n",
        "hf_asr_pipeline = None\n",
        "print(f\"\\n--- 準備 Hugging Face ASR Pipeline ---\")\n",
        "if 'MODEL_ID_FROM_HUB' in globals() and MODEL_ID_FROM_HUB and 'device' in globals():\n",
        "    try:\n",
        "        hf_asr_pipeline = pipeline(\n",
        "            \"automatic-speech-recognition\",\n",
        "            model=MODEL_ID_FROM_HUB,    # 應在儲存格 6 定義 (例如 \"openai/whisper-base\")\n",
        "            device=str(device),         # 應在儲存格 4 定義 (\"cuda\" 或 \"cpu\")\n",
        "            chunk_length_s=30,          # Whisper 通常處理30秒的區塊\n",
        "            # return_timestamps=\"word\"  # 或 \"segment\"，獲取初步的時間分割\n",
        "        )\n",
        "        print(f\"Hugging Face ASR Pipeline ({MODEL_ID_FROM_HUB}) 載入成功。\")\n",
        "    except Exception as e:\n",
        "        print(f\"載入 Hugging Face ASR Pipeline 失敗: {e}\")\n",
        "else:\n",
        "    print(\"錯誤: MODEL_ID_FROM_HUB 或 device 未定義。無法載入 Hugging Face ASR Pipeline。\")\n",
        "\n",
        "\n",
        "# 2. 載入 WhisperX 對齊模型 (只執行一次)\n",
        "align_model_global = None\n",
        "align_metadata_global = None\n",
        "print(f\"\\n--- 準備 WhisperX 對齊模型 ---\")\n",
        "if 'TARGET_LANGUAGE' in globals() and TARGET_LANGUAGE and 'device' in globals():\n",
        "    try:\n",
        "        print(f\"正在為語言 '{TARGET_LANGUAGE}' 載入 WhisperX 對齊模型...\")\n",
        "        align_model_global, align_metadata_global = whisperx.load_align_model(language_code=TARGET_LANGUAGE, device=str(device))\n",
        "        print(\"WhisperX 對齊模型載入成功。\")\n",
        "    except Exception as e:\n",
        "        print(f\"載入 WhisperX 對齊模型失敗 for language {TARGET_LANGUAGE}: {e}\")\n",
        "        print(\"將無法執行精確的時間戳對齊。\")\n",
        "else:\n",
        "    print(\"錯誤: TARGET_LANGUAGE 或 device 未定義。無法載入 WhisperX 對齊模型。\")\n",
        "\n",
        "\n",
        "# --- 定義推斷和儲存函式 ---\n",
        "def calculate_output_with_hf_pipeline_and_align(\n",
        "    asr_pipeline_instance, # 傳入 Hugging Face ASR Pipeline\n",
        "    align_model,\n",
        "    align_meta,\n",
        "    dataloader,\n",
        "    output_version_dir,\n",
        "    target_lang_code=\"en\" # 用於 align 和 pipeline 的 generate_kwargs\n",
        "):\n",
        "    print(\"\\n--- 開始計算輸出 (使用 HF Pipeline + WhisperX Align) ---\")\n",
        "    predictions_texts = []\n",
        "    filenames_list = []\n",
        "    export_json_data = {}\n",
        "\n",
        "    if not asr_pipeline_instance:\n",
        "        print(\"錯誤: Hugging Face ASR Pipeline 未載入，無法進行推斷。\")\n",
        "        return\n",
        "    if not align_model or not align_meta:\n",
        "        print(\"警告: WhisperX 對齊模型未載入，時間戳將不會被精確對齊。\")\n",
        "\n",
        "    for batch in tqdm(dataloader, desc=\"處理音訊檔案\", unit=\"file\"):\n",
        "        audio_file_path = batch[\"file_path\"][0]\n",
        "        audio_file_name = batch[\"audio_file_name\"][0]\n",
        "\n",
        "        full_transcript_for_task1 = f\"ERROR_PROCESSING_{audio_file_name}\"\n",
        "        final_word_segments_for_task2 = []\n",
        "        detected_language_for_json = target_lang_code # pipeline 強制語言\n",
        "\n",
        "        with torch.no_grad():\n",
        "            try:\n",
        "\n",
        "                pipeline_output = asr_pipeline_instance(\n",
        "                    audio_file_path, # Pipeline 可以直接接受檔案路徑\n",
        "                    generate_kwargs={\"language\": target_lang_code, \"task\": \"transcribe\"},\n",
        "                    return_timestamps=\"word\" # 獲取詞級別的初步時間戳和文本塊\n",
        "                )\n",
        "\n",
        "                full_transcript_from_pipeline = pipeline_output[\"text\"]\n",
        "                initial_segments_for_align = [] # 準備給 whisperx.align 的格式\n",
        "\n",
        "                if \"chunks\" in pipeline_output and pipeline_output[\"chunks\"]:\n",
        "                    for chunk in pipeline_output[\"chunks\"]:\n",
        "                        initial_segments_for_align.append({\n",
        "                            \"text\": chunk[\"text\"].strip(), # whisperx.align 需要純文本\n",
        "                            \"start\": chunk[\"timestamp\"][0],\n",
        "                            \"end\": chunk[\"timestamp\"][1]\n",
        "                        })\n",
        "                else: # 如果 pipeline 沒有返回 chunks，我們創建一個包含完整文本的 segment\n",
        "                      # 這時 align 依賴其內部 VAD 或純文本對齊能力\n",
        "                    print(f\"  提示: HF Pipeline 未返回 'chunks' for {audio_file_name}. \"\n",
        "                          \"將使用完整文本進行對齊 (對齊效果可能受影響)。\")\n",
        "\n",
        "                    initial_segments_for_align = [{\"text\": full_transcript_from_pipeline}]\n",
        "\n",
        "\n",
        "                full_transcript_for_task1 = full_transcript_from_pipeline # 預設 Task 1 輸出\n",
        "\n",
        "\n",
        "                if align_model and align_meta and initial_segments_for_align:\n",
        "                    audio_waveform_for_align = whisperx.load_audio(audio_file_path) # align 需要波形\n",
        "                    try:\n",
        "\n",
        "                        alignment = whisperx.align(\n",
        "                            initial_segments_for_align,\n",
        "                            align_model,\n",
        "                            align_meta,\n",
        "                            audio_waveform_for_align,\n",
        "                            device=str(device) # 確保是字串\n",
        "                        )\n",
        "                        full_transcript_for_task1 = alignment.get(\"text\", full_transcript_for_task1) # 優先使用對齊後的文本\n",
        "\n",
        "                        if \"word_segments\" in alignment and alignment[\"word_segments\"]:\n",
        "                            for word_info in alignment[\"word_segments\"]:\n",
        "                                if word_info.get(\"word\") is not None and word_info.get(\"start\") is not None and word_info.get(\"end\") is not None:\n",
        "                                    final_word_segments_for_task2.append({\n",
        "                                        \"text\": word_info.get(\"word\", \"\"),\n",
        "                                        \"timestamp\": (word_info.get(\"start\"), word_info.get(\"end\")),\n",
        "                                        \"score\": word_info.get(\"score\", None)\n",
        "                                    })\n",
        "                        else:\n",
        "                             print(f\"  提示: {audio_file_name} 的 WhisperX Align 結果中未找到 'word_segments'。\")\n",
        "                             # 如果對齊後沒有詞級別，可以嘗試從初步的 pipeline chunks 提取\n",
        "                             if \"chunks\" in pipeline_output and pipeline_output[\"chunks\"]:\n",
        "                                 for chunk in pipeline_output[\"chunks\"]:\n",
        "                                     final_word_segments_for_task2.append({\n",
        "                                         \"text\": chunk[\"text\"].strip(),\n",
        "                                         \"timestamp\": chunk[\"timestamp\"],\n",
        "                                         \"score\": None # HF pipeline 不直接給詞對齊分數\n",
        "                                     })\n",
        "\n",
        "\n",
        "                    except Exception as e_align:\n",
        "                        print(f\"  處理 {audio_file_name} 時 WhisperX Align 失敗: {e_align}. Task 1 使用初步轉錄，Task 2 時間戳將來自初步轉錄 (若有)。\")\n",
        "                        # 如果對齊失敗，嘗試使用 pipeline 的 word timestamps (如果可用)\n",
        "                        if \"chunks\" in pipeline_output and pipeline_output[\"chunks\"]:\n",
        "                            for chunk in pipeline_output[\"chunks\"]:\n",
        "                                final_word_segments_for_task2.append({\n",
        "                                    \"text\": chunk[\"text\"].strip(),\n",
        "                                    \"timestamp\": chunk[\"timestamp\"],\n",
        "                                    \"score\": None\n",
        "                                })\n",
        "                else: # 未執行對齊 (因為模型未載入或無初步 segments)\n",
        "                    print(f\"  提示: 未執行 WhisperX Align for {audio_file_name}。Task 1 使用初步轉錄，Task 2 時間戳將來自初步轉錄 (若有)。\")\n",
        "                    if \"chunks\" in pipeline_output and pipeline_output[\"chunks\"]:\n",
        "                         for chunk in pipeline_output[\"chunks\"]:\n",
        "                            final_word_segments_for_task2.append({\n",
        "                                \"text\": chunk[\"text\"].strip(),\n",
        "                                \"timestamp\": chunk[\"timestamp\"],\n",
        "                                \"score\": None\n",
        "                            })\n",
        "\n",
        "            except Exception as e_pipeline_transcribe:\n",
        "                print(f\"  處理音訊 {audio_file_name} 時 HF Pipeline 轉錄或主流程發生錯誤: {e_pipeline_transcribe}\")\n",
        "                # full_transcript_for_task1 已設為錯誤訊息\n",
        "                final_word_segments_for_task2 = []\n",
        "\n",
        "            predictions_texts.append(full_transcript_for_task1)\n",
        "            filenames_list.append(audio_file_name)\n",
        "            export_json_data[audio_file_name] = {\n",
        "                \"language\": detected_language_for_json,\n",
        "                \"full_text_for_task1\": full_transcript_for_task1,\n",
        "                \"word_timestamps_for_task2\": final_word_segments_for_task2\n",
        "            }\n",
        "\n",
        "    # --- 儲存結果 ---\n",
        "    task1_output_filepath = os.path.join(output_version_dir, \"task1_answer_hf_pipeline_align.txt\")\n",
        "    print(f\"\\n正在儲存 Task 1 的預測 ({len(predictions_texts)} 條) 到: {task1_output_filepath}\")\n",
        "    with open(task1_output_filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "        for pred_text in predictions_texts:\n",
        "            f.write(str(pred_text).strip() + \"\\n\")\n",
        "\n",
        "    json_output_filepath = os.path.join(output_version_dir, \"val_detailed_timestamps_hf_pipeline_align.json\")\n",
        "    print(f\"正在儲存詳細的時間戳資訊到: {json_output_filepath}\")\n",
        "    with open(json_output_filepath, \"w\", encoding=\"utf-8\") as f:\n",
        "        json.dump(export_json_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "    print(f\"所有預測結果和時間戳已儲存。\")\n",
        "\n",
        "\n",
        "# --- 準備 DataLoader 並執行推斷 ---\n",
        "print(\"\\n--- 開始執行推斷流程 (HF Pipeline + WhisperX Align) ---\")\n",
        "\n",
        "if ('processed_dataset' in globals() and 'val' in processed_dataset and\n",
        "    processed_dataset['val'] and len(processed_dataset['val']) > 0 and\n",
        "    'hf_asr_pipeline' in globals() and hf_asr_pipeline and # 改用 hf_asr_pipeline\n",
        "    'align_model_global' in globals() and # 對齊模型是可選的，但最好有\n",
        "    'version_dir' in globals() and version_dir and\n",
        "    'TARGET_LANGUAGE' in globals() and TARGET_LANGUAGE):\n",
        "\n",
        "    columns_to_keep = [\"file_path\", \"audio_file_name\"]\n",
        "    inference_dataset = None\n",
        "    try:\n",
        "        inference_dataset = processed_dataset['val'].select_columns(columns_to_keep)\n",
        "    except ValueError as e:\n",
        "        print(f\"從 processed_dataset['val'] 選擇欄位時出錯: {e}.\")\n",
        "        print(f\"可用欄位: {processed_dataset['val'].column_names if processed_dataset['val'] else 'N/A'}\")\n",
        "\n",
        "    if inference_dataset and len(inference_dataset) > 0 :\n",
        "        eval_dataloader = DataLoader(inference_dataset, batch_size=1)\n",
        "\n",
        "        calculate_output_with_hf_pipeline_and_align(\n",
        "            asr_pipeline_instance=hf_asr_pipeline, # 傳入 HF Pipeline\n",
        "            align_model=align_model_global,\n",
        "            align_meta=align_metadata_global,\n",
        "            dataloader=eval_dataloader,\n",
        "            output_version_dir=version_dir,\n",
        "            target_lang_code=TARGET_LANGUAGE\n",
        "        )\n",
        "    elif not inference_dataset or len(inference_dataset) == 0:\n",
        "         print(\"錯誤: 未能創建包含有效資料的 inference_dataset。無法執行推斷。\")\n",
        "    else: # Dataloader 創建失敗的情況 (較少見如果 inference_dataset 有內容)\n",
        "        print(\"錯誤: DataLoader 未能創建。無法執行推斷。\")\n",
        "else:\n",
        "    print(\"錯誤: 必要變數 (processed_dataset['val'], hf_asr_pipeline, version_dir, TARGET_LANGUAGE) 未準備好或音訊資料為空。\")\n",
        "    print(\"請按順序執行所有之前的儲存格。\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyO9J4KxqIyOetr4S9xDX9AW",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}